{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cell.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cnn import *\n",
    "from data_utils import get_CIFAR10_data\n",
    "from layers import *\n",
    "from fast_layers import *\n",
    "from solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (49000, 3, 32, 32)\n",
      "y_train: (49000,)\n",
      "X_val: (1000, 3, 32, 32)\n",
      "y_val: (1000,)\n",
      "X_test: (1000, 3, 32, 32)\n",
      "y_test: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR-10 data.\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n"
     ]
    }
   ],
   "source": [
    "!python setup.py build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_fast:\n",
      "Naive: 8.833341s\n",
      "Fast: 0.034457s\n",
      "Speedup: 256.355067x\n",
      "Difference:  4.926407851494105e-11\n",
      "\n",
      "Testing conv_backward_fast:\n",
      "Naive: 21.277648s\n",
      "Fast: 0.028924s\n",
      "Speedup: 735.652307x\n",
      "dx difference:  1.949764775345631e-11\n",
      "dw difference:  4.957046344783224e-13\n",
      "db difference:  3.481354613192702e-14\n"
     ]
    }
   ],
   "source": [
    "# Rel errors should be around e-9 or less.\n",
    "from fast_layers import conv_forward_fast, conv_backward_fast\n",
    "from time import time\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 31, 31)\n",
    "w = np.random.randn(25, 3, 3, 3)\n",
    "b = np.random.randn(25,)\n",
    "dout = np.random.randn(100, 25, 16, 16)\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_naive(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing conv_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting conv_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))\n",
    "print('dw difference: ', rel_error(dw_naive, dw_fast))\n",
    "print('db difference: ', rel_error(db_naive, db_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pool_forward_fast:\n",
      "Naive: 0.311715s\n",
      "fast: 0.006980s\n",
      "speedup: 44.660188x\n",
      "difference:  0.0\n",
      "\n",
      "Testing pool_backward_fast:\n",
      "Naive: 2.099463s\n",
      "fast: 0.024934s\n",
      "speedup: 84.200620x\n",
      "dx difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Relative errors should be close to 0.0.\n",
    "from fast_layers import max_pool_forward_fast, max_pool_backward_fast\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(100, 3, 32, 32)\n",
    "dout = np.random.randn(100, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_naive(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print('Testing pool_forward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('fast: %fs' % (t2 - t1))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('difference: ', rel_error(out_naive, out_fast))\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_naive(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print('\\nTesting pool_backward_fast:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('fast: %fs' % (t2 - t1))\n",
    "print('speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('dx difference: ', rel_error(dx_naive, dx_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (no regularization):  2.302586630973189\n",
      "Initial loss (with regularization):  2.508695802231438\n"
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (with regularization): ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 30) loss: 2.670487\n",
      "(Epoch 0 / 15) train acc: 0.200000; val_acc: 0.127000\n",
      "(Iteration 2 / 30) loss: 2.463370\n",
      "(Epoch 1 / 15) train acc: 0.220000; val_acc: 0.122000\n",
      "(Iteration 3 / 30) loss: 3.089000\n",
      "(Iteration 4 / 30) loss: 2.659823\n",
      "(Epoch 2 / 15) train acc: 0.160000; val_acc: 0.107000\n",
      "(Iteration 5 / 30) loss: 2.613649\n",
      "(Iteration 6 / 30) loss: 2.438302\n",
      "(Epoch 3 / 15) train acc: 0.210000; val_acc: 0.089000\n",
      "(Iteration 7 / 30) loss: 2.109035\n",
      "(Iteration 8 / 30) loss: 2.100708\n",
      "(Epoch 4 / 15) train acc: 0.230000; val_acc: 0.108000\n",
      "(Iteration 9 / 30) loss: 2.197319\n",
      "(Iteration 10 / 30) loss: 1.940376\n",
      "(Epoch 5 / 15) train acc: 0.250000; val_acc: 0.120000\n",
      "(Iteration 11 / 30) loss: 1.867510\n",
      "(Iteration 12 / 30) loss: 1.764319\n",
      "(Epoch 6 / 15) train acc: 0.310000; val_acc: 0.134000\n",
      "(Iteration 13 / 30) loss: 2.151846\n",
      "(Iteration 14 / 30) loss: 2.093021\n",
      "(Epoch 7 / 15) train acc: 0.320000; val_acc: 0.160000\n",
      "(Iteration 15 / 30) loss: 1.735965\n",
      "(Iteration 16 / 30) loss: 1.721776\n",
      "(Epoch 8 / 15) train acc: 0.400000; val_acc: 0.149000\n",
      "(Iteration 17 / 30) loss: 1.615951\n",
      "(Iteration 18 / 30) loss: 1.698681\n",
      "(Epoch 9 / 15) train acc: 0.370000; val_acc: 0.180000\n",
      "(Iteration 19 / 30) loss: 1.719933\n",
      "(Iteration 20 / 30) loss: 1.790876\n",
      "(Epoch 10 / 15) train acc: 0.430000; val_acc: 0.159000\n",
      "(Iteration 21 / 30) loss: 1.436579\n",
      "(Iteration 22 / 30) loss: 1.467298\n",
      "(Epoch 11 / 15) train acc: 0.470000; val_acc: 0.159000\n",
      "(Iteration 23 / 30) loss: 1.327970\n",
      "(Iteration 24 / 30) loss: 1.673823\n",
      "(Epoch 12 / 15) train acc: 0.460000; val_acc: 0.175000\n",
      "(Iteration 25 / 30) loss: 1.618815\n",
      "(Iteration 26 / 30) loss: 1.617136\n",
      "(Epoch 13 / 15) train acc: 0.510000; val_acc: 0.173000\n",
      "(Iteration 27 / 30) loss: 1.346362\n",
      "(Iteration 28 / 30) loss: 1.387480\n",
      "(Epoch 14 / 15) train acc: 0.510000; val_acc: 0.188000\n",
      "(Iteration 29 / 30) loss: 1.473237\n",
      "(Iteration 30 / 30) loss: 1.333378\n",
      "(Epoch 15 / 15) train acc: 0.510000; val_acc: 0.177000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(weight_scale=1e-2)\n",
    "\n",
    "solver = Solver(\n",
    "    model,\n",
    "    small_data,\n",
    "    num_epochs=15,\n",
    "    batch_size=50,\n",
    "    update_rule='adam',\n",
    "    optim_config={'learning_rate': 1e-3,},\n",
    "    verbose=True,\n",
    "    print_every=1\n",
    ")\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small data training accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "# Print final training accuracy.\n",
    "print(\n",
    "    \"Small data training accuracy:\",\n",
    "    solver.check_accuracy(small_data['X_train'], small_data['y_train'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small data validation accuracy: 0.185\n"
     ]
    }
   ],
   "source": [
    "# Print final validation accuracy.\n",
    "print(\n",
    "    \"Small data validation accuracy:\",\n",
    "    solver.check_accuracy(small_data['X_val'], small_data['y_val'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 6) loss: 2.323173\n",
      "(Epoch 0 / 3) train acc: 0.140000; val_acc: 0.108000\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.105000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.124000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.146000\n",
      "(Iteration 1 / 6) loss: 2.343790\n",
      "(Epoch 0 / 3) train acc: 0.130000; val_acc: 0.081000\n",
      "(Epoch 1 / 3) train acc: 0.130000; val_acc: 0.081000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.130000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.142000\n",
      "(Iteration 1 / 6) loss: 2.384688\n",
      "(Epoch 0 / 3) train acc: 0.200000; val_acc: 0.123000\n",
      "(Epoch 1 / 3) train acc: 0.210000; val_acc: 0.144000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.108000\n",
      "(Epoch 3 / 3) train acc: 0.370000; val_acc: 0.167000\n",
      "(Iteration 1 / 6) loss: 2.508200\n",
      "(Epoch 0 / 3) train acc: 0.130000; val_acc: 0.108000\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.120000\n",
      "(Epoch 2 / 3) train acc: 0.110000; val_acc: 0.113000\n",
      "(Epoch 3 / 3) train acc: 0.160000; val_acc: 0.104000\n",
      "(Iteration 1 / 3) loss: 2.323238\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.075000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.113000\n",
      "(Epoch 3 / 3) train acc: 0.200000; val_acc: 0.107000\n",
      "(Iteration 1 / 3) loss: 2.343879\n",
      "(Epoch 1 / 3) train acc: 0.100000; val_acc: 0.109000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.120000\n",
      "(Epoch 3 / 3) train acc: 0.220000; val_acc: 0.119000\n",
      "(Iteration 1 / 3) loss: 2.384652\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.129000\n",
      "(Epoch 2 / 3) train acc: 0.230000; val_acc: 0.128000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.135000\n",
      "(Iteration 1 / 3) loss: 2.507663\n",
      "(Epoch 1 / 3) train acc: 0.130000; val_acc: 0.079000\n",
      "(Epoch 2 / 3) train acc: 0.200000; val_acc: 0.149000\n",
      "(Epoch 3 / 3) train acc: 0.200000; val_acc: 0.147000\n",
      "(Iteration 1 / 3) loss: 2.323265\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.126000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.129000\n",
      "(Epoch 3 / 3) train acc: 0.170000; val_acc: 0.145000\n",
      "(Iteration 1 / 3) loss: 2.343752\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.200000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.220000; val_acc: 0.141000\n",
      "(Iteration 1 / 3) loss: 2.384866\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.120000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.146000\n",
      "(Iteration 1 / 3) loss: 2.507799\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.120000; val_acc: 0.105000\n",
      "(Epoch 3 / 3) train acc: 0.130000; val_acc: 0.105000\n",
      "(Iteration 1 / 3) loss: 2.323372\n",
      "(Epoch 1 / 3) train acc: 0.210000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.190000; val_acc: 0.127000\n",
      "(Epoch 3 / 3) train acc: 0.240000; val_acc: 0.149000\n",
      "(Iteration 1 / 3) loss: 2.343866\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.142000\n",
      "(Iteration 1 / 3) loss: 2.385044\n",
      "(Epoch 1 / 3) train acc: 0.110000; val_acc: 0.113000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.118000\n",
      "(Epoch 3 / 3) train acc: 0.240000; val_acc: 0.133000\n",
      "(Iteration 1 / 3) loss: 2.507535\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.140000; val_acc: 0.109000\n",
      "(Epoch 3 / 3) train acc: 0.170000; val_acc: 0.109000\n",
      "(Iteration 1 / 6) loss: 2.302815\n",
      "(Epoch 0 / 3) train acc: 0.080000; val_acc: 0.121000\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.110000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.126000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.128000\n",
      "(Iteration 1 / 6) loss: 2.302902\n",
      "(Epoch 0 / 3) train acc: 0.160000; val_acc: 0.120000\n",
      "(Epoch 1 / 3) train acc: 0.170000; val_acc: 0.130000\n",
      "(Epoch 2 / 3) train acc: 0.220000; val_acc: 0.127000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.144000\n",
      "(Iteration 1 / 6) loss: 2.303507\n",
      "(Epoch 0 / 3) train acc: 0.170000; val_acc: 0.115000\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.106000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.127000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.140000\n",
      "(Iteration 1 / 6) loss: 2.305471\n",
      "(Epoch 0 / 3) train acc: 0.140000; val_acc: 0.106000\n",
      "(Epoch 1 / 3) train acc: 0.170000; val_acc: 0.115000\n",
      "(Epoch 2 / 3) train acc: 0.240000; val_acc: 0.114000\n",
      "(Epoch 3 / 3) train acc: 0.220000; val_acc: 0.120000\n",
      "(Iteration 1 / 3) loss: 2.302807\n",
      "(Epoch 1 / 3) train acc: 0.140000; val_acc: 0.110000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.118000\n",
      "(Iteration 1 / 3) loss: 2.302945\n",
      "(Epoch 1 / 3) train acc: 0.170000; val_acc: 0.114000\n",
      "(Epoch 2 / 3) train acc: 0.230000; val_acc: 0.115000\n",
      "(Epoch 3 / 3) train acc: 0.300000; val_acc: 0.117000\n",
      "(Iteration 1 / 3) loss: 2.303414\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 3 / 3) train acc: 0.130000; val_acc: 0.092000\n",
      "(Iteration 1 / 3) loss: 2.304453\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.130000; val_acc: 0.080000\n",
      "(Epoch 3 / 3) train acc: 0.080000; val_acc: 0.070000\n",
      "(Iteration 1 / 3) loss: 2.302730\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.115000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.137000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.147000\n",
      "(Iteration 1 / 3) loss: 2.303010\n",
      "(Epoch 1 / 3) train acc: 0.180000; val_acc: 0.121000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 3 / 3) train acc: 0.260000; val_acc: 0.125000\n",
      "(Iteration 1 / 3) loss: 2.303517\n",
      "(Epoch 1 / 3) train acc: 0.170000; val_acc: 0.118000\n",
      "(Epoch 2 / 3) train acc: 0.200000; val_acc: 0.128000\n",
      "(Epoch 3 / 3) train acc: 0.260000; val_acc: 0.124000\n",
      "(Iteration 1 / 3) loss: 2.304704\n",
      "(Epoch 1 / 3) train acc: 0.140000; val_acc: 0.126000\n",
      "(Epoch 2 / 3) train acc: 0.140000; val_acc: 0.104000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.118000\n",
      "(Iteration 1 / 3) loss: 2.302809\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.140000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.180000; val_acc: 0.123000\n",
      "(Iteration 1 / 3) loss: 2.302896\n",
      "(Epoch 1 / 3) train acc: 0.170000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.120000\n",
      "(Epoch 3 / 3) train acc: 0.280000; val_acc: 0.148000\n",
      "(Iteration 1 / 3) loss: 2.303385\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.120000\n",
      "(Epoch 2 / 3) train acc: 0.210000; val_acc: 0.153000\n",
      "(Epoch 3 / 3) train acc: 0.280000; val_acc: 0.145000\n",
      "(Iteration 1 / 3) loss: 2.304789\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.130000; val_acc: 0.108000\n",
      "(Epoch 3 / 3) train acc: 0.180000; val_acc: 0.101000\n",
      "(Iteration 1 / 6) loss: 2.302565\n",
      "(Epoch 0 / 3) train acc: 0.170000; val_acc: 0.114000\n",
      "(Epoch 1 / 3) train acc: 0.130000; val_acc: 0.106000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.095000\n",
      "(Epoch 3 / 3) train acc: 0.240000; val_acc: 0.138000\n",
      "(Iteration 1 / 6) loss: 2.302707\n",
      "(Epoch 0 / 3) train acc: 0.140000; val_acc: 0.103000\n",
      "(Epoch 1 / 3) train acc: 0.190000; val_acc: 0.120000\n",
      "(Epoch 2 / 3) train acc: 0.200000; val_acc: 0.130000\n",
      "(Epoch 3 / 3) train acc: 0.270000; val_acc: 0.141000\n",
      "(Iteration 1 / 6) loss: 2.302729\n",
      "(Epoch 0 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.096000\n",
      "(Epoch 2 / 3) train acc: 0.130000; val_acc: 0.101000\n",
      "(Epoch 3 / 3) train acc: 0.200000; val_acc: 0.130000\n",
      "(Iteration 1 / 6) loss: 2.302785\n",
      "(Epoch 0 / 3) train acc: 0.100000; val_acc: 0.102000\n",
      "(Epoch 1 / 3) train acc: 0.200000; val_acc: 0.110000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.108000\n",
      "(Epoch 3 / 3) train acc: 0.220000; val_acc: 0.097000\n",
      "(Iteration 1 / 3) loss: 2.302559\n",
      "(Epoch 1 / 3) train acc: 0.110000; val_acc: 0.128000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.092000\n",
      "(Epoch 3 / 3) train acc: 0.170000; val_acc: 0.107000\n",
      "(Iteration 1 / 3) loss: 2.302568\n",
      "(Epoch 1 / 3) train acc: 0.180000; val_acc: 0.110000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.128000\n",
      "(Epoch 3 / 3) train acc: 0.260000; val_acc: 0.146000\n",
      "(Iteration 1 / 3) loss: 2.302819\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.116000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.138000\n",
      "(Iteration 1 / 3) loss: 2.302994\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.114000\n",
      "(Epoch 2 / 3) train acc: 0.120000; val_acc: 0.076000\n",
      "(Epoch 3 / 3) train acc: 0.120000; val_acc: 0.104000\n",
      "(Iteration 1 / 3) loss: 2.302478\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.125000\n",
      "(Epoch 2 / 3) train acc: 0.190000; val_acc: 0.125000\n",
      "(Epoch 3 / 3) train acc: 0.240000; val_acc: 0.129000\n",
      "(Iteration 1 / 3) loss: 2.302750\n",
      "(Epoch 1 / 3) train acc: 0.140000; val_acc: 0.121000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.126000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.141000\n",
      "(Iteration 1 / 3) loss: 2.302782\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.120000\n",
      "(Epoch 2 / 3) train acc: 0.240000; val_acc: 0.153000\n",
      "(Epoch 3 / 3) train acc: 0.270000; val_acc: 0.145000\n",
      "(Iteration 1 / 3) loss: 2.302877\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.100000; val_acc: 0.088000\n",
      "(Epoch 3 / 3) train acc: 0.130000; val_acc: 0.079000\n",
      "(Iteration 1 / 3) loss: 2.302665\n",
      "(Epoch 1 / 3) train acc: 0.140000; val_acc: 0.111000\n",
      "(Epoch 2 / 3) train acc: 0.110000; val_acc: 0.116000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.124000\n",
      "(Iteration 1 / 3) loss: 2.302633\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.116000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.155000\n",
      "(Iteration 1 / 3) loss: 2.302592\n",
      "(Epoch 1 / 3) train acc: 0.180000; val_acc: 0.102000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.240000; val_acc: 0.141000\n",
      "(Iteration 1 / 3) loss: 2.302999\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.130000; val_acc: 0.100000\n",
      "(Epoch 3 / 3) train acc: 0.110000; val_acc: 0.066000\n",
      "(Iteration 1 / 6) loss: 2.302628\n",
      "(Epoch 0 / 3) train acc: 0.160000; val_acc: 0.123000\n",
      "(Epoch 1 / 3) train acc: 0.180000; val_acc: 0.123000\n",
      "(Epoch 2 / 3) train acc: 0.100000; val_acc: 0.156000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.137000\n",
      "(Iteration 1 / 6) loss: 2.302692\n",
      "(Epoch 0 / 3) train acc: 0.160000; val_acc: 0.118000\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.190000; val_acc: 0.113000\n",
      "(Epoch 3 / 3) train acc: 0.260000; val_acc: 0.145000\n",
      "(Iteration 1 / 6) loss: 2.302415\n",
      "(Epoch 0 / 3) train acc: 0.060000; val_acc: 0.128000\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.121000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.116000\n",
      "(Epoch 3 / 3) train acc: 0.170000; val_acc: 0.129000\n",
      "(Iteration 1 / 6) loss: 2.302350\n",
      "(Epoch 0 / 3) train acc: 0.150000; val_acc: 0.116000\n",
      "(Epoch 1 / 3) train acc: 0.130000; val_acc: 0.121000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.103000\n",
      "(Epoch 3 / 3) train acc: 0.280000; val_acc: 0.113000\n",
      "(Iteration 1 / 3) loss: 2.302634\n",
      "(Epoch 1 / 3) train acc: 0.140000; val_acc: 0.095000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.129000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.133000\n",
      "(Iteration 1 / 3) loss: 2.302570\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.115000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 3 / 3) train acc: 0.200000; val_acc: 0.128000\n",
      "(Iteration 1 / 3) loss: 2.302614\n",
      "(Epoch 1 / 3) train acc: 0.080000; val_acc: 0.090000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.118000\n",
      "(Epoch 3 / 3) train acc: 0.200000; val_acc: 0.155000\n",
      "(Iteration 1 / 3) loss: 2.302452\n",
      "(Epoch 1 / 3) train acc: 0.130000; val_acc: 0.106000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.117000\n",
      "(Epoch 3 / 3) train acc: 0.170000; val_acc: 0.101000\n",
      "(Iteration 1 / 3) loss: 2.302524\n",
      "(Epoch 1 / 3) train acc: 0.180000; val_acc: 0.117000\n",
      "(Epoch 2 / 3) train acc: 0.190000; val_acc: 0.117000\n",
      "(Epoch 3 / 3) train acc: 0.270000; val_acc: 0.147000\n",
      "(Iteration 1 / 3) loss: 2.302541\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.120000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.099000\n",
      "(Epoch 3 / 3) train acc: 0.300000; val_acc: 0.148000\n",
      "(Iteration 1 / 3) loss: 2.302713\n",
      "(Epoch 1 / 3) train acc: 0.140000; val_acc: 0.112000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.119000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.122000\n",
      "(Iteration 1 / 3) loss: 2.302612\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.140000; val_acc: 0.118000\n",
      "(Epoch 3 / 3) train acc: 0.100000; val_acc: 0.097000\n",
      "(Iteration 1 / 3) loss: 2.302591\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.120000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.136000\n",
      "(Iteration 1 / 3) loss: 2.302582\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.124000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.120000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.132000\n",
      "(Iteration 1 / 3) loss: 2.302425\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.200000; val_acc: 0.140000\n",
      "(Epoch 3 / 3) train acc: 0.280000; val_acc: 0.147000\n",
      "(Iteration 1 / 3) loss: 2.302477\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.094000\n",
      "(Epoch 3 / 3) train acc: 0.180000; val_acc: 0.119000\n",
      "(Iteration 1 / 6) loss: 2.323396\n",
      "(Epoch 0 / 3) train acc: 0.130000; val_acc: 0.098000\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.126000\n",
      "(Epoch 2 / 3) train acc: 0.140000; val_acc: 0.131000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.145000\n",
      "(Iteration 1 / 6) loss: 2.343623\n",
      "(Epoch 0 / 3) train acc: 0.120000; val_acc: 0.107000\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.111000\n",
      "(Epoch 2 / 3) train acc: 0.260000; val_acc: 0.142000\n",
      "(Epoch 3 / 3) train acc: 0.270000; val_acc: 0.135000\n",
      "(Iteration 1 / 6) loss: 2.384411\n",
      "(Epoch 0 / 3) train acc: 0.150000; val_acc: 0.091000\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.097000\n",
      "(Epoch 3 / 3) train acc: 0.140000; val_acc: 0.089000\n",
      "(Iteration 1 / 6) loss: 2.508175\n",
      "(Epoch 0 / 3) train acc: 0.170000; val_acc: 0.118000\n",
      "(Epoch 1 / 3) train acc: 0.190000; val_acc: 0.112000\n",
      "(Epoch 2 / 3) train acc: 0.290000; val_acc: 0.161000\n",
      "(Epoch 3 / 3) train acc: 0.220000; val_acc: 0.109000\n",
      "(Iteration 1 / 3) loss: 2.323432\n",
      "(Epoch 1 / 3) train acc: 0.130000; val_acc: 0.118000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.118000\n",
      "(Epoch 3 / 3) train acc: 0.160000; val_acc: 0.122000\n",
      "(Iteration 1 / 3) loss: 2.343896\n",
      "(Epoch 1 / 3) train acc: 0.180000; val_acc: 0.110000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.127000\n",
      "(Epoch 3 / 3) train acc: 0.210000; val_acc: 0.143000\n",
      "(Iteration 1 / 3) loss: 2.384538\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.115000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.093000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.127000\n",
      "(Iteration 1 / 3) loss: 2.507800\n",
      "(Epoch 1 / 3) train acc: 0.130000; val_acc: 0.079000\n",
      "(Epoch 2 / 3) train acc: 0.150000; val_acc: 0.109000\n",
      "(Epoch 3 / 3) train acc: 0.240000; val_acc: 0.127000\n",
      "(Iteration 1 / 3) loss: 2.323449\n",
      "(Epoch 1 / 3) train acc: 0.140000; val_acc: 0.095000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.113000\n",
      "(Epoch 3 / 3) train acc: 0.180000; val_acc: 0.140000\n",
      "(Iteration 1 / 3) loss: 2.343766\n",
      "(Epoch 1 / 3) train acc: 0.110000; val_acc: 0.113000\n",
      "(Epoch 2 / 3) train acc: 0.210000; val_acc: 0.138000\n",
      "(Epoch 3 / 3) train acc: 0.310000; val_acc: 0.153000\n",
      "(Iteration 1 / 3) loss: 2.384591\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.134000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.133000\n",
      "(Epoch 3 / 3) train acc: 0.260000; val_acc: 0.134000\n",
      "(Iteration 1 / 3) loss: 2.508072\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.230000; val_acc: 0.103000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.104000\n",
      "(Iteration 1 / 3) loss: 2.323234\n",
      "(Epoch 1 / 3) train acc: 0.150000; val_acc: 0.118000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.240000; val_acc: 0.149000\n",
      "(Iteration 1 / 3) loss: 2.343832\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.118000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.290000; val_acc: 0.165000\n",
      "(Iteration 1 / 3) loss: 2.385062\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.120000\n",
      "(Epoch 3 / 3) train acc: 0.180000; val_acc: 0.100000\n",
      "(Iteration 1 / 3) loss: 2.507739\n",
      "(Epoch 1 / 3) train acc: 0.110000; val_acc: 0.089000\n",
      "(Epoch 2 / 3) train acc: 0.200000; val_acc: 0.112000\n",
      "(Epoch 3 / 3) train acc: 0.110000; val_acc: 0.102000\n",
      "(Iteration 1 / 6) loss: 2.302867\n",
      "(Epoch 0 / 3) train acc: 0.130000; val_acc: 0.109000\n",
      "(Epoch 1 / 3) train acc: 0.210000; val_acc: 0.135000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.104000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.126000\n",
      "(Iteration 1 / 6) loss: 2.302958\n",
      "(Epoch 0 / 3) train acc: 0.080000; val_acc: 0.101000\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.085000\n",
      "(Epoch 2 / 3) train acc: 0.140000; val_acc: 0.118000\n",
      "(Epoch 3 / 3) train acc: 0.220000; val_acc: 0.164000\n",
      "(Iteration 1 / 6) loss: 2.303484\n",
      "(Epoch 0 / 3) train acc: 0.160000; val_acc: 0.085000\n",
      "(Epoch 1 / 3) train acc: 0.170000; val_acc: 0.104000\n",
      "(Epoch 2 / 3) train acc: 0.210000; val_acc: 0.126000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.134000\n",
      "(Iteration 1 / 6) loss: 2.305011\n",
      "(Epoch 0 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.099000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.190000; val_acc: 0.144000\n",
      "(Iteration 1 / 3) loss: 2.302715\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.118000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.118000\n",
      "(Epoch 3 / 3) train acc: 0.150000; val_acc: 0.116000\n",
      "(Iteration 1 / 3) loss: 2.303018\n",
      "(Epoch 1 / 3) train acc: 0.190000; val_acc: 0.146000\n",
      "(Epoch 2 / 3) train acc: 0.160000; val_acc: 0.134000\n",
      "(Epoch 3 / 3) train acc: 0.140000; val_acc: 0.077000\n",
      "(Iteration 1 / 3) loss: 2.303341\n",
      "(Epoch 1 / 3) train acc: 0.160000; val_acc: 0.119000\n",
      "(Epoch 2 / 3) train acc: 0.190000; val_acc: 0.126000\n",
      "(Epoch 3 / 3) train acc: 0.260000; val_acc: 0.116000\n",
      "(Iteration 1 / 3) loss: 2.304421\n",
      "(Epoch 1 / 3) train acc: 0.180000; val_acc: 0.125000\n",
      "(Epoch 2 / 3) train acc: 0.130000; val_acc: 0.114000\n",
      "(Epoch 3 / 3) train acc: 0.130000; val_acc: 0.107000\n",
      "(Iteration 1 / 3) loss: 2.302744\n",
      "(Epoch 1 / 3) train acc: 0.190000; val_acc: 0.115000\n",
      "(Epoch 2 / 3) train acc: 0.180000; val_acc: 0.124000\n",
      "(Epoch 3 / 3) train acc: 0.250000; val_acc: 0.145000\n",
      "(Iteration 1 / 3) loss: 2.302999\n",
      "(Epoch 1 / 3) train acc: 0.120000; val_acc: 0.111000\n",
      "(Epoch 2 / 3) train acc: 0.170000; val_acc: 0.121000\n",
      "(Epoch 3 / 3) train acc: 0.230000; val_acc: 0.119000\n",
      "(Iteration 1 / 3) loss: 2.303414\n",
      "(Epoch 1 / 3) train acc: 0.200000; val_acc: 0.117000\n",
      "(Epoch 2 / 3) train acc: 0.200000; val_acc: 0.125000\n",
      "(Epoch 3 / 3) train acc: 0.140000; val_acc: 0.122000\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "model_param = {}\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_model = None\n",
    "\n",
    "learning_rates = [9e-4, 1e-3, 3e-3]\n",
    "reg_strength = [0.1, 0.001, 0.0001, 0.000001] \n",
    "batch_size = [50,100,300,500]\n",
    "hidden_dim = [50, 100, 200, 500]\n",
    "\n",
    "for lr in learning_rates:\n",
    "  for rs in reg_strength:\n",
    "    for bs in batch_size:\n",
    "      for hd in hidden_dim:\n",
    "        model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=hd, reg=rs)\n",
    "        solver = Solver(model, small_data, num_epochs=3, batch_size=bs, update_rule='adam', \n",
    "        optim_config={'learning_rate': lr,}, verbose=True, print_every=2   0)\n",
    "        solver.train()\n",
    "        valid_accuracy = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        train_accuracy = solver.check_accuracy(data['X_train'], data['y_train'])\n",
    "        \n",
    "        if valid_accuracy > best_val:\n",
    "          best_val = valid_accuracy\n",
    "          best_model = solver\n",
    "          model_param = {lr,rs,bs,hd}\n",
    "\"\"\"\n",
    "for lr, rs in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, rs)]\n",
    "    print('lr %e rs %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, rs, train_accuracy, val_accuracy))\n",
    "\"\"\"\n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)\n",
    "print(model_param)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 980) loss: 2.304740\n",
      "(Epoch 0 / 1) train acc: 0.103000; val_acc: 0.107000\n",
      "(Iteration 21 / 980) loss: 2.098229\n",
      "(Iteration 41 / 980) loss: 1.949788\n",
      "(Iteration 61 / 980) loss: 1.888398\n",
      "(Iteration 81 / 980) loss: 1.877093\n",
      "(Iteration 101 / 980) loss: 1.851877\n",
      "(Iteration 121 / 980) loss: 1.859353\n",
      "(Iteration 141 / 980) loss: 1.800181\n",
      "(Iteration 161 / 980) loss: 2.143292\n",
      "(Iteration 181 / 980) loss: 1.830573\n",
      "(Iteration 201 / 980) loss: 2.037280\n",
      "(Iteration 221 / 980) loss: 2.020304\n",
      "(Iteration 241 / 980) loss: 1.823728\n",
      "(Iteration 261 / 980) loss: 1.692679\n",
      "(Iteration 281 / 980) loss: 1.882594\n",
      "(Iteration 301 / 980) loss: 1.798261\n",
      "(Iteration 321 / 980) loss: 1.851960\n",
      "(Iteration 341 / 980) loss: 1.716323\n",
      "(Iteration 361 / 980) loss: 1.897655\n",
      "(Iteration 381 / 980) loss: 1.319744\n",
      "(Iteration 401 / 980) loss: 1.738790\n",
      "(Iteration 421 / 980) loss: 1.488866\n",
      "(Iteration 441 / 980) loss: 1.718409\n",
      "(Iteration 461 / 980) loss: 1.744440\n",
      "(Iteration 481 / 980) loss: 1.605460\n",
      "(Iteration 501 / 980) loss: 1.494847\n",
      "(Iteration 521 / 980) loss: 1.835179\n",
      "(Iteration 541 / 980) loss: 1.483923\n",
      "(Iteration 561 / 980) loss: 1.676871\n",
      "(Iteration 581 / 980) loss: 1.438325\n",
      "(Iteration 601 / 980) loss: 1.443469\n",
      "(Iteration 621 / 980) loss: 1.529369\n",
      "(Iteration 641 / 980) loss: 1.763475\n",
      "(Iteration 661 / 980) loss: 1.790329\n",
      "(Iteration 681 / 980) loss: 1.693343\n",
      "(Iteration 701 / 980) loss: 1.637078\n",
      "(Iteration 721 / 980) loss: 1.644564\n",
      "(Iteration 741 / 980) loss: 1.708919\n",
      "(Iteration 761 / 980) loss: 1.494252\n",
      "(Iteration 781 / 980) loss: 1.901751\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\projects\\ConvolutionalNetworks\\ConvolutionalNeuralNet.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/projects/ConvolutionalNetworks/ConvolutionalNeuralNet.ipynb#ch0000014?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m ThreeLayerConvNet(weight_scale\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, hidden_dim\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, reg\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/projects/ConvolutionalNetworks/ConvolutionalNeuralNet.ipynb#ch0000014?line=2'>3</a>\u001b[0m solver \u001b[39m=\u001b[39m Solver(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/projects/ConvolutionalNetworks/ConvolutionalNeuralNet.ipynb#ch0000014?line=3'>4</a>\u001b[0m     model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/projects/ConvolutionalNetworks/ConvolutionalNeuralNet.ipynb#ch0000014?line=4'>5</a>\u001b[0m     data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/projects/ConvolutionalNetworks/ConvolutionalNeuralNet.ipynb#ch0000014?line=10'>11</a>\u001b[0m     print_every\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/projects/ConvolutionalNetworks/ConvolutionalNeuralNet.ipynb#ch0000014?line=11'>12</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/User/Desktop/projects/ConvolutionalNetworks/ConvolutionalNeuralNet.ipynb#ch0000014?line=12'>13</a>\u001b[0m solver\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\projects\\ConvolutionalNetworks\\solver.py:263\u001b[0m, in \u001b[0;36mSolver.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m num_iterations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_epochs \u001b[39m*\u001b[39m iterations_per_epoch\n\u001b[0;32m    262\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m--> 263\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step()\n\u001b[0;32m    265\u001b[0m     \u001b[39m# Maybe print training loss\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39mand\u001b[39;00m t \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\projects\\ConvolutionalNetworks\\solver.py:188\u001b[0m, in \u001b[0;36mSolver._step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m dw \u001b[39m=\u001b[39m grads[p]\n\u001b[0;32m    187\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_configs[p]\n\u001b[1;32m--> 188\u001b[0m next_w, next_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_rule(w, dw, config)\n\u001b[0;32m    189\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparams[p] \u001b[39m=\u001b[39m next_w\n\u001b[0;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_configs[p] \u001b[39m=\u001b[39m next_config\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\projects\\ConvolutionalNetworks\\optim.py:161\u001b[0m, in \u001b[0;36madam\u001b[1;34m(w, dw, config)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39m###########################################################################\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39m# TODO: Implement the Adam update formula, storing the next value of w in #\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39m# the next_w variable. Don't forget to update the m, v, and t variables   #\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m###########################################################################\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\u001b[39;00m\n\u001b[0;32m    159\u001b[0m config[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \n\u001b[1;32m--> 161\u001b[0m config[\u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mbeta1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39;49m\u001b[39m-\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mbeta1\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m*\u001b[39;49mdw\n\u001b[0;32m    162\u001b[0m config[\u001b[39m'\u001b[39m\u001b[39mv\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mbeta2\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mv\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mbeta2\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m*\u001b[39m(dw\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    163\u001b[0m mb \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mbeta1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet(weight_scale=0.001, hidden_dim=500, reg=0.001)\n",
    "\n",
    "solver = Solver(\n",
    "    model,\n",
    "    data,\n",
    "    num_epochs=1,\n",
    "    batch_size=50,\n",
    "    update_rule='adam',\n",
    "    optim_config={'learning_rate': 1e-3,},\n",
    "    verbose=True,\n",
    "    print_every=20\n",
    ")\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final training accuracy.\n",
    "print(\n",
    "    \"Full data training accuracy:\",\n",
    "    solver.check_accuracy(data['X_train'], data['y_train'])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95a22eaad15357b8581a3e85dc7deb44b392b83f2a0c98c7572069d11874febe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
